\documentclass[11pt]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue
}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  columns=fullflexible,
  frame=single,
  backgroundcolor=\color{gray!5}
}

\title{Numerical Programming Final Project (Task 3)\\Dynamic Tracking and Shape Preservation}
\author{Student: \textbf{(Andria Gvaramia)} \quad Course: Numerical Programming \quad KIU}
\date{\today}

\begin{document}
\maketitle

\section*{Task statement}
\textbf{Input:} drone swarm at the New Year greeting (Task 2 final formation) and a video of choice.\\
\textbf{Goal:} move the swarm from the greeting to a moving object in the video and dynamically repeat the object's motion with shape preservation.\\
\textbf{Output:} trajectory of each drone and visualization whose input is the trajectory.

\section{Overview}
Task 3 is implemented in \texttt{task3/dynamic\_tracking.py}. The key idea is:
\begin{itemize}
  \item Extract the moving object's silhouette from each video frame (multiple segmentation options).
  \item Compute the silhouette \textbf{boundary/contour}.
  \item Sample $N$ points on the contour at \textbf{equal arc-length spacing}.
  \item Maintain a \textbf{stable correspondence} so each drone stays attached to a particular contour point as the object moves.
  \item Visualize drones over the video (or on a blank background) as a GIF.
\end{itemize}

\section{Inputs}
\subsection{Start formation}
The start formation is the greeting formation (Task 2 targets):
\begin{lstlisting}
task2/outputs/target_points.csv
\end{lstlisting}
The number of drones is $N=\texttt{len(start)}$ (default: 150 in our run).

\subsection{Video}
We use:
\begin{lstlisting}
task3/video.mp4
\end{lstlisting}
The video contains a dark silhouette on a bright green background, which makes the green-screen segmentation robust, but other segmenters are supported as well.

\section{Mathematical model}
\subsection{State}
We work in 2D pixel coordinates. Drone $i$ has position:
\[
x_i(t)\in\mathbb{R}^2,\quad i=1,\dots,N.
\]

\subsection{Contour targets (shape preservation)}
For each frame $k$ we compute a closed contour curve $C_k$ of the moving object.
We define $N$ target points by equal arc-length sampling:
\[
T_i^{(k)} = C_k(s_i),\quad s_i=\frac{i}{N}\,L(C_k),\quad i=0,\dots,N-1,
\]
where $L(C_k)$ is the contour perimeter length.
This produces a point set that lies \textbf{only on the boundary} (required condition).

\subsection{Stable ``attachment'' across frames}
Raw contour sampling has an arbitrary start index and orientation. To keep drone $i$ attached to a consistent boundary point, we align the sequence
by choosing a stable start and by minimizing a cyclic shift mismatch:
\[
\min_{\text{shift }s}\sum_{i=0}^{N-1}\left\|T_i^{(k-1)} - T_{(i+s)\bmod N}^{(k)}\right\|^2,
\]
also checking the reversed order (to handle orientation flips).

\section{Numerical methods}
\subsection{Segmentation}
We provide multiple segmentation methods:
\begin{itemize}
  \item \textbf{greenscreen}: HSV thresholding for green background.
  \item \textbf{mog2}: background subtraction (MOG2) for generic videos.
  \item \textbf{edges}: Canny edges + contour fill.
\end{itemize}
All methods apply morphological cleanup and ignore a small bottom strip (\texttt{--ignore-bottom-frac}) to avoid spurious contours.

\subsection{High-detail contour extraction}
To increase boundary precision, we extract contours on an upscaled mask:
\begin{lstlisting}
--contour-upscale 3.0 --contour-smooth 9
\end{lstlisting}
Upscaling yields subpixel-level detail once scaled back, and smoothing removes jagged contour noise.

\subsection{Time discretization}
Frames define a natural time step:
\[
\Delta t = \frac{\texttt{video\_step}}{\texttt{fps}}.
\]
Using \texttt{--video-step 1} uses every frame and maximizes temporal accuracy.

\section{Drone motion controller}
Two controllers are supported:
\begin{itemize}
  \item \textbf{Direct (perfect tracking)}: $x_i^{(k)} = T_i^{(k)}$. This produces \textbf{zero tracking error} and is ideal for visualization quality.
  \item \textbf{Dynamics (2nd-order)}: a damped point-mass model integrated with RK4 to follow moving targets, which introduces lag.
\end{itemize}
The best result used direct tracking:
\begin{lstlisting}
--controller direct
\end{lstlisting}

\section{Reproducibility (commands)}
All commands run from the project root.

\subsection{Video + drones (best result)}
\begin{lstlisting}
python3 task3/dynamic_tracking.py \
  --tracking-mode contour \
  --controller direct \
  --video-step 1 \
  --contour-upscale 3.0 --contour-smooth 9 \
  --segmenter greenscreen \
  --save-gif --save-traj-csv --save-traj-npy --gif-fps 30 \
  --output-gif task3/outputs/task3_contour_direct_hi_with_video.gif
\end{lstlisting}

\subsection{Drones-only GIF (blank background)}
\begin{lstlisting}
python3 task3/dynamic_tracking.py \
  --tracking-mode contour \
  --controller direct \
  --video-step 1 \
  --contour-upscale 3.0 --contour-smooth 9 \
  --segmenter greenscreen \
  --no-bg --drone-color blue \
  --save-gif --save-traj-csv --save-traj-npy --gif-fps 30 \
  --output-gif task3/outputs/task3_contour_direct_hi_drones_only_blue.gif
\end{lstlisting}

\section{Outputs}
Generated files (reproducible):
\begin{itemize}
  \item \texttt{task3/outputs/task3\_contour\_direct\_hi\_with\_video.gif}
  \item \texttt{task3/outputs/task3\_contour\_direct\_hi\_drones\_only.gif}
  \item \texttt{task3/outputs/task3\_contour\_direct\_hi\_drones\_only\_blue.gif}
  \item \texttt{task3/outputs/task3\_trajectories.csv}
  \item \texttt{task3/outputs/task3\_trajectories.npy}
\end{itemize}

\section{Validation and limitations}
\subsection{Validation}
The primary validation criterion is the constraint:
\[
x_i^{(k)} \in \partial \Omega_k
\]
where $\partial\Omega_k$ is the object boundary at frame $k$.
In \texttt{direct} mode, this holds by construction because drones are set to the contour samples.

\subsection{Limitations}
\begin{itemize}
  \item Contour tracking quality depends on segmentation; complex backgrounds may require tuning the selected segmenter.
  \item Using dynamics (instead of direct tracking) introduces lag and can violate the strict boundary constraint.
  \item With fixed $N$, the maximum geometric detail is limited by the number of sampled points on the contour.
\end{itemize}

\section{AI usage disclosure}
This project was developed with AI assistance (ChatGPT) used for:
\begin{itemize}
  \item explaining tracking / contour correspondence ideas,
  \item proposing numerical pipeline improvements and parameter choices,
  \item helping implement and debug Python code and visualization.
\end{itemize}

\end{document}

